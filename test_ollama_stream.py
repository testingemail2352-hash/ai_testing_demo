import ollama


def test_ollama_stream():
    model_name = "mistral:7b"
    prompt = "Scrie 'Salut!' È™i adaugÄƒ un mic mesaj de bun venit."

    try:
        print("ğŸ¤– Test Ollama (streaming)...")
        response_stream = ollama.chat(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )

        # ItereazÄƒ È™i afiÈ™eazÄƒ output-ul pe mÄƒsurÄƒ ce vine
        for chunk in response_stream:
            if 'content' in chunk:
                print(chunk['content'], end='', flush=True)

        print("\nâœ… Streaming complet!")

    except Exception as e:
        print("âŒ Eroare la conectarea cu Ollama:")
        print(e)


if __name__ == "__main__":
    test_ollama_stream()
